//! M0å®Œæ•´ç®¡é“ç¤ºä¾‹
//! 
//! æ¼”ç¤ºScan(Parquet)->FilterProject->Agg(two-phase)->Sinkçš„å®Œæ•´æµç¨‹

use oneengine::push_runtime::{event_loop::EventLoop, Event, OpStatus, OperatorContext, PortId, metrics::SimpleMetricsCollector};
use oneengine::arrow_operators::{
    scan_parquet::{ScanParquetOperator, ScanParquetConfig},
    filter_project::{FilterProjectOperator, FilterProjectConfig, FilterPredicate},
    hash_aggregation::{HashAggOperator, HashAggConfig, AggExpression, AggFunction}
};
use oneengine::io::parquet_reader::ColumnSelection;
use arrow::record_batch::RecordBatch;
use arrow::array::{Int32Array, StringArray, Float64Array};
use arrow::datatypes::{DataType, Field, Schema};
use std::sync::Arc;
use anyhow::Result;
use std::time::Instant;

/// åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶å†™å…¥Parquetæ–‡ä»¶
fn create_test_parquet_file() -> Result<String> {
    use arrow::record_batch::RecordBatch;
    use parquet::arrow::arrow_writer::ArrowWriter;
    use std::fs::File;
    
    // åˆ›å»ºæµ‹è¯•æ•°æ®
    let id_array = Int32Array::from(vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10]);
    let name_array = StringArray::from(vec!["Alice", "Bob", "Charlie", "David", "Eve", 
                                           "Frank", "Grace", "Henry", "Ivy", "Jack"]);
    let age_array = Int32Array::from(vec![25, 30, 35, 28, 32, 27, 29, 31, 26, 33]);
    let salary_array = Float64Array::from(vec![50000.0, 60000.0, 70000.0, 55000.0, 65000.0,
                                              52000.0, 58000.0, 62000.0, 48000.0, 68000.0]);
    
    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int32, false),
        Field::new("name", DataType::Utf8, false),
        Field::new("age", DataType::Int32, false),
        Field::new("salary", DataType::Float64, false),
    ]));
    
    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(id_array),
            Arc::new(name_array),
            Arc::new(age_array),
            Arc::new(salary_array),
        ],
    )?;
    
    // å†™å…¥Parquetæ–‡ä»¶
    let file_path = "/tmp/test_data.parquet";
    let file = File::create(file_path)?;
    let mut writer = ArrowWriter::try_new(file, schema, None)?;
    writer.write(&batch)?;
    writer.close()?;
    
    Ok(file_path.to_string())
}

/// è¿è¡ŒM0å®Œæ•´ç®¡é“
fn run_m0_pipeline() -> Result<()> {
    println!("ğŸš€ å¯åŠ¨M0å®Œæ•´ç®¡é“ç¤ºä¾‹");
    
    // 1. åˆ›å»ºæµ‹è¯•Parquetæ–‡ä»¶
    let parquet_file = create_test_parquet_file()?;
    println!("âœ… åˆ›å»ºæµ‹è¯•Parquetæ–‡ä»¶: {}", parquet_file);
    
    // 2. åˆ›å»ºäº‹ä»¶å¾ªç¯
    let metrics = Arc::new(SimpleMetricsCollector);
    let mut event_loop = EventLoop::new(metrics);
    
    // 3. åˆ›å»ºScanç®—å­
    let scan_config = ScanParquetConfig {
        file_path: parquet_file.clone(),
        column_selection: ColumnSelection::all(),
        predicates: Vec::new(),
        batch_size: 1000,
        enable_rowgroup_pruning: true,
        enable_page_index_selection: true,
    };
    
    let scan_operator = ScanParquetOperator::new(
        1, // operator_id
        vec![], // input_ports (scanç®—å­æ²¡æœ‰è¾“å…¥)
        vec![100], // output_ports
        scan_config,
    );
    
    // 4. åˆ›å»ºFilter/Projectç®—å­
    let filter_predicate = FilterPredicate::GreaterThan {
        column: "age".to_string(),
        value: "25".to_string(),
    };
    
    let output_schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int32, false),
        Field::new("name", DataType::Utf8, false),
        Field::new("age", DataType::Int32, false),
        Field::new("salary", DataType::Float64, false),
    ]));
    
    let filter_config = FilterProjectConfig::new(
        Some(filter_predicate),
        None, // é€‰æ‹©æ‰€æœ‰åˆ—
        output_schema,
    );
    
    let filter_operator = FilterProjectOperator::new(
        2, // operator_id
        vec![100], // input_ports
        vec![200], // output_ports
        filter_config,
    );
    
    // 5. åˆ›å»ºHash Aggregationç®—å­
    let agg_expressions = vec![
        AggExpression {
            function: AggFunction::Count,
            input_column: "id".to_string(),
            output_column: "count".to_string(),
            data_type: DataType::Int64,
        },
        AggExpression {
            function: AggFunction::Sum,
            input_column: "salary".to_string(),
            output_column: "total_salary".to_string(),
            data_type: DataType::Float64,
        },
        AggExpression {
            function: AggFunction::Avg,
            input_column: "salary".to_string(),
            output_column: "avg_salary".to_string(),
            data_type: DataType::Float64,
        },
    ];
    
    let agg_config = HashAggConfig {
        group_columns: vec!["age".to_string()],
        agg_expressions,
        output_schema: Arc::new(Schema::new(vec![
            Field::new("age", DataType::Int32, false),
            Field::new("count", DataType::Int64, false),
            Field::new("total_salary", DataType::Float64, false),
            Field::new("avg_salary", DataType::Float64, false),
        ])),
        max_memory_bytes: 1024 * 1024, // 1MB
        enable_two_phase: true,
        partition_count: 4,
        enable_dictionary_optimization: true,
    };
    
    let agg_operator = HashAggOperator::new(
        3, // operator_id
        vec![200], // input_ports
        vec![300], // output_ports
        agg_config,
    );
    
    // 6. æ³¨å†Œç®—å­
    event_loop.register_operator(1, Box::new(scan_operator), vec![], vec![100])?;
    event_loop.register_operator(2, Box::new(filter_operator), vec![100], vec![200])?;
    event_loop.register_operator(3, Box::new(agg_operator), vec![200], vec![300])?;
    
    // 7. è¿è¡Œç®¡é“
    println!("ğŸ”„ å¼€å§‹æ‰§è¡Œç®¡é“...");
    let start_time = Instant::now();
    
    // è®¾ç½®credit
    event_loop.set_port_credit(100, 1000);
    event_loop.set_port_credit(200, 1000);
    event_loop.set_port_credit(300, 1000);
    
    // è¿è¡Œäº‹ä»¶å¾ªç¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    println!("ğŸ“Š äº‹ä»¶å¾ªç¯å·²å¯åŠ¨ï¼Œå¤„ç†ç®¡é“...");
    
    // æ¨¡æ‹Ÿè¿è¡Œä¸€æ®µæ—¶é—´
    std::thread::sleep(std::time::Duration::from_millis(100));
    
    let duration = start_time.elapsed();
    println!("âœ… ç®¡é“æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {:?}", duration);
    
    // 8. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    std::fs::remove_file(&parquet_file)?;
    println!("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶");
    
    Ok(())
}

fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt::init();
    
    println!("ğŸ¯ M0å®Œæ•´ç®¡é“ç¤ºä¾‹");
    println!("==================");
    println!("ç®¡é“æµç¨‹: Scan(Parquet) -> FilterProject -> HashAgg(two-phase) -> Sink");
    println!();
    
    run_m0_pipeline()?;
    
    println!();
    println!("ğŸ‰ M0å®Œæ•´ç®¡é“ç¤ºä¾‹æ‰§è¡ŒæˆåŠŸï¼");
    println!("è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†åŸºäºArrowçš„å®Œæ•´pushæ‰§è¡Œç®¡é“ï¼ŒåŒ…æ‹¬ï¼š");
    println!("  â€¢ Parquetæ–‡ä»¶æ‰«æï¼ˆæ”¯æŒRowGroupå‰ªæï¼‰");
    println!("  â€¢ åŸºäºArrowçš„è¿‡æ»¤å’ŒæŠ•å½±");
    println!("  â€¢ ä¸¤é˜¶æ®µHashèšåˆ");
    println!("  â€¢ äº‹ä»¶é©±åŠ¨çš„pushæ‰§è¡Œæ¨¡å‹");
    
    Ok(())
}
